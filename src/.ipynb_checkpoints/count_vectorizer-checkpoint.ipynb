{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "# import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Import stopwords with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Chrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Chrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files/Tesseract-OCR/tesseract'\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all images in dir to jpg\n",
    "def to_format(ext, to_ext, inp, out, label):\n",
    "    print('enter')\n",
    "    \n",
    "    amount = len(glob.glob('{dir}/*.{ext}'.format(dir=inp, ext=ext)))\n",
    "    print('amount files: ' + str(amount))\n",
    "    for im, i in zip(glob.glob('{dir}/*.{ext}'.format(dir=inp, ext=ext)), range(amount)):\n",
    "        \n",
    "        try:\n",
    "            im = Image.open(im)\n",
    "            im = im.convert('RGB')\n",
    "            im.save('{output}/{label}{id}.{ext}'.format(output=out, label=label, id=i, ext=to_ext))\n",
    "            print('saving img to folder {output}'.format(idx=i+1, id=1, ext=to_ext, output=out))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "           \n",
    "       # print(\"finished writing {percentage} percent...\".format(percentage=math.ceil((100/len(labels))*(idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "amount files: 4\n",
      "saving img to folder ./data/lieferbeleg/\n",
      "saving img to folder ./data/lieferbeleg/\n",
      "saving img to folder ./data/lieferbeleg/\n",
      "saving img to folder ./data/lieferbeleg/\n"
     ]
    }
   ],
   "source": [
    "to_format('png','jpg', './data/lieferbeleg/raw', './data/lieferbeleg/', 'lieferbeleg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(im):\n",
    "    \"\"\"\n",
    "    binarize image to improve accuracy\n",
    "    \"\"\"\n",
    "    _ ,im = cv2.threshold(np.array(im),127,255,cv2.THRESH_BINARY)\n",
    "    return Image.fromarray(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text(im):\n",
    "    \"\"\"\n",
    "    extract text of given image\n",
    "    :param im: image to extract text from\n",
    "    \"\"\"\n",
    "    im = Image.open(im)\n",
    "    im = preprocess(im)\n",
    "    \n",
    "    extracted = pytesseract.image_to_string(im, output_type=Output.DICT, lang='deu')\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_list(input_dir, label):\n",
    "    \n",
    "    res = []\n",
    "    ext = \"jpg\"\n",
    "    images = glob.glob('{dir}/*.{ext}'.format(dir=input_dir, ext=ext))\n",
    "    print('amount files: ' + str(amount))\n",
    "    for im, i in zip(images, range(len(amount))):\n",
    "        extracted_text = extract_text(im)\n",
    "        extracted_text['id'] = i +1\n",
    "        extracted_text['label'] = label\n",
    "        res.append(extracted_text)\n",
    "   \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount files: 5\n",
      "amount files: 6\n"
     ]
    }
   ],
   "source": [
    "list_rb = texts_to_list('./data/rechnungsbeleg', label='invoice')\n",
    "list_lb = texts_to_list('./data/lieferbeleg', label='delivery')\n",
    "\n",
    "with open('./data/rechnungsbeleg/invoice_data.json', 'w') as fout:\n",
    "    json.dump(list_rb, fout)\n",
    "with open('./data/lieferbeleg/delivery_data.json', 'w') as fout:\n",
    "    json.dump(list_lb , fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read json into a dataframe\n",
    "df_inv=pd.read_json(\"./data/rechnungsbeleg/invoice_data.json\")\n",
    "df_del=pd.read_json(\"./data/lieferbeleg/delivery_data.json\")\n",
    "\n",
    "df = pd.concat([df_inv, df_del])\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    [Mustermann, Malerei, GmbH, Hauptstrasse, 123,...\n",
       "2    [362, TRAP, HOM, usterste, 112, Hummer, Holdin...\n",
       "1    [M, Carsten, Hilgers, Carsten, Hilgers, Stahlw...\n",
       "4    [Firmenname, Frmenname, Musterstraie, 57, 1234...\n",
       "4    [o, 8, r, na, easyblll, Dukum, nt, ifach, rech...\n",
       "0    [Muster, Straße, 1, 12345, Musterstadt, Muster...\n",
       "3                   [da4d, Muster, GmbH, Lieferschein]\n",
       "3    [Dr, Susanne, Jetzi, Physiotherapie, Mustergas...\n",
       "5    [Firmenname, Musterstraße, 5, 12345, Stadt, Fi...\n",
       "1    [unitymedia, Unitymedia, Kunden, Service, Cent...\n",
       "0    [Lieferschein, Druckwiederhe, rg, 10003839, 33...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p = inflect.engine()\n",
    "\n",
    "# remove special characters\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(\"(\\\\W)+\",\" \",x))\n",
    "\n",
    "# remove punctuation\n",
    "df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)) \n",
    "    \n",
    "# tokenize \n",
    "df['text'] = df['text'].apply(lambda x:nltk.word_tokenize(x))\n",
    "\n",
    "# to lower case\n",
    "df['text'].apply(lambda x: [word.lower() for word in x])    \n",
    "\n",
    "# filter special characters\n",
    "df['text'].apply(lambda x: [word.lower() for word in x])    \n",
    "\n",
    "# filter stopwords\n",
    "df['text'].apply(lambda x: [item for item in x if item not in stopwords.words('german')])\n",
    "\n",
    "# remove punctuation\n",
    "df['text'].apply(lambda x: [re.sub(r'[^\\w\\s]', '', word) for word in x if word != '']) \n",
    "\n",
    "# Remove non-ASCII characters from list of tokenized words\n",
    "df['text'].apply(lambda x: [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in x])    \n",
    "\n",
    "# Replace all interger occurrences in list of tokenized words with textual representation\n",
    "df['text'].apply(lambda x: [p.number_to_words(word) for word in x if word.isdigit()])\n",
    "  \n",
    "# stemming\n",
    "df['text'].apply(lambda x: [stemmer.stem(word) for word in x])  \n",
    "\n",
    "# lemmatizing\n",
    "df['text'].apply(lambda x: [lemmatizer.lemmatize(word, pos='v') for word in x])    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "#train, test = train_test_split(df, test_size=0.2)\n",
    "#test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x2c47b9f36a0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "# create a count vectorizer object disable preprocessing and tokenizing \n",
    "count_vect = CountVectorizer(\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    ")  \n",
    "\n",
    "# count_vect.fit(X_train)\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape\n",
    "plt.spy(X_train_counts, markersize=1)\n",
    "\n",
    "#sn.set(font_scale=1.4)#for label size\n",
    "#sn.heatmap(X_train_counts, annot=True,annot_kws={\"size\": 16})# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x573 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 718 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(tokenizer=dummy,preprocessor=dummy,)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf = text_clf.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    delivery       1.00      1.00      1.00         1\n",
      "     invoice       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "predicted\n",
    "np.mean(predicted == y_test)\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "filename = 'text_clf.sav'\n",
    "joblib.dump(text_clf, filename)\n",
    " \n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
