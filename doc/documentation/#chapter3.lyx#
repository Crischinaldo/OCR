#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrbook
\options bibliography=totoc,listof=totoc,BCOR=5mm,DIV=12
\use_default_options false
\maintain_unincluded_children false
\language ngerman
\language_package default
\inputencoding iso8859-1
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 0
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine biblatex
\cite_engine_type authoryear
\biblio_style plainnat
\biblio_options backend=biber
\biblatex_bibstyle alphabetic
\biblatex_citestyle alphabetic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style german
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
chapter{Entwicklung des Textklassifikatoren}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chap_architecture_classificator"

\end_inset


\end_layout

\begin_layout Standard
Dieses Kapitel befasst sich mit der Entwicklung des Klassifikators und ist
 in den zwei Kapiteln Architektur und Implementierung gegliedert.
\end_layout

\begin_layout Section
Datenanalyse
\end_layout

\begin_layout Standard
Unter der Organisation der Daten ist sowohl die Formatierung als auch die
 Strukturierung von den zur Verfügung stehenden Datensätzen gemeint.
 Um das Model mit Überwachtem Lernen zu trainieren müssen die Daten so strukturi
ert sein, dass die Kategorie von jeden Datenpunkt bekannt ist.
 Ursprünglich sind die Bilder im PDF-Format in einer Ordnerstruktur gegliedert
 gewesen, die abhängig vom Hochladedatum nach Jahr, Monat und Tag sortiert
 wurden sind.
 Die Zeitangabe für die Textextraktion ist als unrelevant zu werten, da
 hier nur auf der inhaltliche Aspekt eine Rolle spielt.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Subsubsection
Kollektion
\end_layout

\begin_layout Standard
Mit dem Programmcode sind alle Dateien kollektiviert, indem rekursiv über
 jeden Ordner in einem bestimmten Ordner
\emph on
 input_dir 
\emph default
alle PDF-Dateien erfasst wurden und in den Ordner 
\emph on
output_dir 
\emph default
verlagert wurden sind.
 Die Generierung der Merkmale verläuft über das Extrahieren der Texte aus
 Bilddateien, weswegen jedes Bild mit der Funktion 
\emph on
convert_from_path()
\emph default
 der externen Bibliothek 
\emph on
pdf2image
\emph default
 zum JPG-Format konvertiert wurden ist.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "caption={Sammeln der Daten},captionpos=b"
inline false
status open

\begin_layout Plain Layout

def collect_data(input_dir, output_dir):
\end_layout

\begin_layout Plain Layout

    ext = 'jpg'
\end_layout

\begin_layout Plain Layout

    for pdf in glob.glob('{inp}/*/**/*.pdf'.format(inp=input_dir), recursive=True):
       
\end_layout

\begin_layout Plain Layout

        try:             
\end_layout

\begin_layout Plain Layout

        	pages = convert_from_path(pdf)         
\end_layout

\begin_layout Plain Layout

		except (ValueError, Exception):             
\end_layout

\begin_layout Plain Layout

    		pass      
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

    for page in pages:             
\end_layout

\begin_layout Plain Layout

   	 page.save('./{output}/{name}.{ext}'.format(
\end_layout

\begin_layout Plain Layout

             output=output_dir, name=str(uuid.uuid4())),  ext=ext)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Die rekursive Suche verlief über glob (
\emph on
Unix style pathname pattern expansion
\emph default
), einer Bibliothek die alle Pfadnamen nach einem spezifischen Muster nach
 Unix-Regeln durchsucht.
 Die Funktion 
\emph on
convert_from_path() 
\emph default
gibt eine Liste Bild-Objekte zurück, da PDF-Dokumente eine Mehrzahl an Seiten
 aufweisen können.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Die Betrachung der Bilder zeigt, dass es sowohl digitalisierte Aufnahmen
 von Sowohl Schadensfotos als auch Belegen aller Art gibt.
 Es wurden zwei Belegarten selektiert, die es zu klassifizieren gilt:
\end_layout

\begin_layout Itemize
Rechnungen
\end_layout

\begin_layout Itemize
Regressforderungen
\end_layout

\begin_layout Standard
Aus Zeitgründen werden hier lediglich zwei Kategorien betrachtet, wobei
 die eigentliche Menge an Daten dafür ausreicht eine Vielzahl von Belegarten
 zu kategorisieren.
\end_layout

\begin_layout Subsubsection
Sortierung
\end_layout

\begin_layout Standard
Eine interessante Möglichkeit die Bilder zu Sortieren ist nach Schlüsselwörtern
 in den Texten der Dokumente zu suchen und sie abhängig davon in bestimmte
 Ordner zu sortieren.
 Durch eine Textextraktion von jedem Bildes gelang es die Texte nach bestimmten
 Schlüsselwörtern zu durchsuchen und sie danach zu sortieren.
 Da eine große Variation an Daten vorhanden ist, sind mitunter auch Ausreißer
 unter den sortierten Daten dabei.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Ausreißer üben eine negative Korrelation auf die Zielsicherheit des Klassifikato
r aus.
 Durch stichprobenartiges herüberschauen ist aufgefallen, dass die Menge
 an Ausreißern überschaubar ist.
 Die Sortierung hat dazu geführt, dass insgesamt 700 Rechnungsbelege und
 700 Regressforderungen identifiziert und kategorisiert wurden.
\end_layout

\begin_layout Section
Vorbereitung der Daten
\end_layout

\begin_layout Standard
Um Die Daten für die Klassifizierung vorzubereiten mussten die Daten in
 einem bestimmtes Format gebracht werden.
 Im Programmcode wird vom Ordner, der durch den Parameter 
\emph on
input_dir 
\emph default
projiziert ist, von der Funktion 
\emph on
extract_text() 
\emph default
der Text extrahiert.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "caption={Formatieren der Daten},captionpos=b"
inline false
status open

\begin_layout Plain Layout

def extract_text(im):     
\end_layout

\begin_layout Plain Layout

	im = Image.open(im)           
\end_layout

\begin_layout Plain Layout

	extracted = pytesseract.image_to_string(im, output_type=Output.DICT, lang='deu')
     
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

	return extracted
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

def doc_to_list(input_dir, label):
\end_layout

\begin_layout Plain Layout

	res = []     
\end_layout

\begin_layout Plain Layout

	ext = "jpg"     
\end_layout

\begin_layout Plain Layout

	images = len(glob.glob('{dir}/*.{ext}'.format(dir=input_dir, ext=ext))   
\end_layout

\begin_layout Plain Layout

	for im, i in zip(images, range(len(images))):
\end_layout

\begin_layout Plain Layout

    	extracted_text = extract_text(im)         
\end_layout

\begin_layout Plain Layout

		extracted_text['id'] = i + 1         
\end_layout

\begin_layout Plain Layout

		extracted_text['label'] = label         
\end_layout

\begin_layout Plain Layout

		res.append(extracted_text)         
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

	return res
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Die Methode 
\emph on
doc_to_list
\series bold
\emph default
 
\series default
fügt dem Dictionary, welches durch 
\emph on
image_to_string()
\emph default
 zurückgegeben wurden ist, eine ID und die Kategorie hinzu und gibt eine
 Liste von Dictionaries zurück.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Mit dem Aufruf 
\emph on
json.dump() 
\emph default
wurden die Listen in ein Json-Array überführt und in einer Json-Datei zwischenge
speichert um sie zu persistieren.
 Mit 
\emph on
Pandas
\begin_inset Foot
status open

\begin_layout Plain Layout

\emph on
https://pandas.pydata.org/
\end_layout

\end_inset


\emph default
 wurden die Json-Dateien wiederrum in einem Dataframe
\begin_inset Foot
status open

\begin_layout Plain Layout
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html
\end_layout

\end_inset

 überführt.
 Ein Dataframe ist eine einer Zweidimensionale tabellarischen Datenstruktur,
 die sich gut visualieren lässt und sich somit gut für Datenanalysen eignet.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "caption={Persistieren der Daten},captionpos=b"
inline false
status open

\begin_layout Plain Layout

with open('./data/rechnungsbeleg/invoice_data.json', 'w') as json_file:  
   	
\end_layout

\begin_layout Plain Layout

	json.dump(list_rb, json_file) 
\end_layout

\begin_layout Plain Layout

with open('./data/regress/regress_data.json', 'w') as json_file:     	
\end_layout

\begin_layout Plain Layout

	json.dump(list_rg, json_file)
\end_layout

\end_inset


\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Die jeweiligen Dataframes werden konkateniert, was dazu führt, dass sich
 alle Texte mit ihrer zugehörigen Kategorie in einem Dataframe befinden.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "caption={Lesen der Json-Dateien},captionpos=b"
inline false
status open

\begin_layout Plain Layout

df_inv = pd.read_json("./data/rechnungsbeleg/invoice_data.json") 
\end_layout

\begin_layout Plain Layout

df_re = pd.read_json("./data/regress/regress_data.json") 
\end_layout

\begin_layout Plain Layout

df = pd.concat([df_inv, df_re]) 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Die Daten wurden mit diesem Vorgang in das Programm geladen und stehen für
 weitere Prozeduren zur Verfügung.
 Die Texte wurden durch die Vorbereitung in eine übersichtliche Struktur
 verlagert, die es erlaubt die Texte im Programm kontrolliert aufzubereiten.
\end_layout

\begin_layout Section
Aufbereitung der Daten
\end_layout

\begin_layout Standard
Die Texte befinden sich momentan noch im Rohzustand, was bedeutet, dass
 sie für das Training ungeeignet sind.
 Im jetzigen Zustand der Daten befinden sich Unregelmäßigkeiten in den Daten,
 was zu einem deutlich Qualitätsverlust führt.
 Beispielsweise beinhalten die Texte willkürliche Zahlen, Zeilenumbrüche
 und unbrauchbare Werte wie Maßeinheiten.
 Die Unregelmäßigkeiten beschränken sich nicht auf diesen Anwendungsfall
 und treten in verschiedenster Form auf.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Der Bereinigungsprozess wird gut vom Machine Learning Team
\emph on
 Azure
\emph default
 von Microsoft
\begin_inset Foot
status open

\begin_layout Plain Layout
https://docs.microsoft.com/de-de/azure/machine-learning/team-data-science-process/
prepare-data
\end_layout

\end_inset

 dokumentiert.
 In der Dokumentation wird beschrieben, dass es drei Kriterien von minderwertige
n Daten gibt: Die Daten können unvollständig durch fehlende Attribute und
 Werte, überflüssig durch fehlerhafte Datensätze und Ausreißer und inkonsistent
 durch wiedersprüchliche Datensätze sein.
 Für die Bereinigung von Texten trifft folgendes Zitat zu:
\end_layout

\begin_layout Standard
[...]
\emph on
 Entfernen eingebetteter Zeichen, die zu einer falschen Datenausrichtung
 führen können, z.
 B.
 eingebetteter Tabstopps in tabstoppgetrennten Dateien oder eingebetteter
 Zeilenumbrüche, die Datensätze unterbrechen könnten 
\emph default
[...]
\begin_inset Foot
status open

\begin_layout Plain Layout
https://docs.microsoft.com/de-de/azure/
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Subsection
Bereinigung der Daten
\end_layout

\begin_layout Standard
Zum Bereinigen der Daten werden auf Text ausgerichtete Verfahren
\begin_inset CommandInset citation
LatexCommand cite
key "A10"
literal "false"

\end_inset

 angewandt:
\end_layout

\begin_layout Subsubsection*
Stopwörter
\end_layout

\begin_layout Standard
Stopwörter sind Wörter die in einer Kollektion von Dokumenten so häufig
 vorkommen, dass es kein Sinn macht sie in das Training mit einzubinden,
 da sie keine Relevanz auf Dokumentinhalte enthalten.
 Typische Wörter sind Artikel, Konjunktionen und Präpositionen.
 Abhängig von den Inhalten der Dokumente können spezifische Stopwörter hinzugefü
gt werden.
\end_layout

\begin_layout Subsubsection
Lemmatisierung
\end_layout

\begin_layout Standard
Es wird eine Reduktion der Wortformen auf ihre Grundform vorgenommen.
 Dieser Vorgang wird mithilfe eines digitalen Lexikons realisiert.
\end_layout

\begin_layout Subsubsection*
Stemming
\end_layout

\begin_layout Standard
Stemming ist ein Verfahren womit Suffixe von dem jeweiligen Wort zu entfernen.
 Suffixe sind generell auf die Englische Sprache reduziert, weshalb diese
 Algorithmen nur für englischsprachige Dokumente eingesetzt werden kann.
\end_layout

\begin_layout Standard
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard
Daneben können verschiedene Algorithmen angewandt werden, wie das ersetzen
 der Großbuchstab durch Kleinbuchstaben oder das Entfernen von Sonderzeichen.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Die Funktion 
\emph on
normalize() 
\emph default
im Programmcode wendet verschiedene Algorithmen auf die Wörter des Textes
 an, um die Daten zu normalisieren.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "caption={Bereinigung der Daten},captionpos=b"
inline false
status open

\begin_layout Plain Layout

def normalize(df):
\end_layout

\begin_layout Plain Layout

	# remove special characters
\end_layout

\begin_layout Plain Layout

	df['text'].apply(lambda x: re.sub("(
\backslash

\backslash
W)+"," ",x))     
\end_layout

\begin_layout Plain Layout

	# remove punctuation     
\end_layout

\begin_layout Plain Layout

	df['text'].apply(lambda x: re.sub(r'[^
\backslash
w
\backslash
s]', '', x))      
\end_layout

\begin_layout Plain Layout

	# tokenize      
\end_layout

\begin_layout Plain Layout

	df['text'].apply(lambda x: nltk.word_tokenize(x))     
\end_layout

\begin_layout Plain Layout

	# to lower case
\end_layout

\begin_layout Plain Layout

	df['text'].apply(lambda x: [word.lower() for word in x])         
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Mit zwei Regex-Ausdrücken wird der Text auf Sonderzeichen und Zeichensetzung
 gefiltert.
 Mit der NLP-Bibliothek 
\emph on
nltk 
\emph default
wird der Text in eine Liste von Tokens konvertiert.
 Dadurch kann jedes Wort iteriert werden um Großbuchstaben in Kleinbuchstaben
 umzuwandeln.
 Lemmatisierung wird nicht eingesetzt, da es die Genauigkeit verringert.
\end_layout

\begin_layout Standard
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Subsection
Trainings- und Testdaten
\end_layout

\begin_layout Standard
Die Daten werden mit der Funktion
\emph on
 train_test_split() 
\emph default
der Bibliothek 
\emph on
sklearn
\begin_inset Foot
status open

\begin_layout Plain Layout

\emph on
https://scikit-learn.org/stable/
\end_layout

\end_inset

, 
\emph default
die allgemein Funktionen für den Bereich des Data Science zur Verfügung
 stellt, in Trainings- und Testmengen unterteilt.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "caption={Unterteilung der Daten},captionpos=b"
inline false
status open

\begin_layout Plain Layout

X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'],
 test_size=0.2)
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
Als Parameter nimmt die Funktion die Beispieltexte und die Kategorien als
 Listen, somit kann die Funktion die Texte zu ihrer jeweiligen Kategorie
 zuordnen.
 Über den Parameter 
\emph on
test_size 
\emph default
wird das Verhältnis von den Teilmengen der Trainings- und Testdaten festgelegt.
 Die Beispieltexte werden gemäß dem Faktor 0.2 in 20 Prozent Testdaten und
 80 Prozent Trainingsdaten unterteilt.
\end_layout

\begin_layout Section
Erstellung des Klassifikationsmodells
\end_layout

\begin_layout Subsection
Vokabular definieren
\end_layout

\begin_layout Standard
Der CountVektorizer
\begin_inset Foot
status open

\begin_layout Plain Layout
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.Co
untVectorizer.html
\end_layout

\end_inset

 von sklearn bietet eine einfache Möglichkeit Texte in Token zu formatieren
 und dadurch ein Vokabular bekannter Wörter zu erstellen.
 Durch Übergabe der Trainingsdaten werden die Aufkommen jedes Wortes gezählt.
 Zusätzlich können Stopwörter und Token Muster übergeben werden, um die
 Zählprozedur zu beeinflussen.
\end_layout

\begin_layout Standard
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard
Die Bibliothek nltk bietet die Möglichkeit sprachabhängig definierte Stopwörter
 als Liste im Programm zu nutzen.
 Diese werden durch benutzerdefinierte Stopwörter ergänzt.
\end_layout

\begin_layout Standard
Die Konstante 
\emph on
token_pattern 
\emph default
definiert einen Regulären Ausdruck mit dem Zweck, dass numerische Token
 nicht mit in die Zählung einfließen.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "caption={Initialisierung und Anpassung des Zählvektors},captionpos=b"
inline false
status open

\begin_layout Plain Layout

nltk.download('stopwords')
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

stop_words = set(stopwords.words("german"))
\end_layout

\begin_layout Plain Layout

new_stopwords_list = stop_words.union(additional_stopwords) 
\end_layout

\begin_layout Plain Layout

token_pattern = r'
\backslash
b[^
\backslash
d
\backslash
W]+
\backslash
b'
\end_layout

\begin_layout Plain Layout

count_vect = CountVectorizer(stop_words = new_stopwords_list, 
\end_layout

\begin_layout Plain Layout

                             token_pattern=token_pattern, 
\end_layout

\begin_layout Plain Layout

                             analyzer='word', 
\end_layout

\begin_layout Plain Layout

                             max_features=30)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

docs = count_vect.fit_transform(X_train) 
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
Die Instanz des Objekts
\emph on
 CountVektorizer
\emph default
 wird mit den vordefinierten Stopwörtern, den vordefinierten regulären Ausdruck
 für die Tokens, einem Indikator für die Erstellung von Charakteristiken
 aus Wörtern und das Limit an Vokabular initialisiert.
\end_layout

\begin_layout Standard
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard
Die Instanz erhält die Trainingsdaten zum Erlernen eines Vokabulars und
 gibt eine Matrix mit 1120 Zeilen und 30 Spalten zurück, welche die Texte
 aus dem Trainingsdaten (Anzahl X_train) mit dem jeweilig gezählten Vokabular
 repräsentieren.
\end_layout

\begin_layout Subsection
Termfrequenz und inverse Dokumentenhäufigkeit
\end_layout

\begin_layout Standard
Das Verfahren zur Berechnung des Tf-idf-Maßes fügt sich dem Bereich der
 Informationsbeschaffung (
\emph on
Information Retrieval
\emph default
).
\end_layout

\begin_layout Standard
Die Termfrequenz Tf (
\emph on
term frequency
\emph default
) gibt an wie häufig ein Term in einem Dokument vorkommt wobei die inverse
 Dokumentenhäufigkeit idf (
\emph on
inverse document frequency
\emph default
) dessen Relevanz im Dokumentkorpus bestimmt.
 Jeder Term hat seine eigene TF- und IDF-Bewertung, die als Gewichtungen
 für das Produkt zur Berechnung des Tf-idf-Maßes verwendet werden 
\begin_inset CommandInset citation
LatexCommand cite
key "R03"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Das Tf-idf-Maß eines Terms ist ein Indikator für dessen seltenes Aufkommen
 eines Dokuments einer Dokumentenkollektion.
 Ein Term welches in wenigen Dokumenten oft vorkommt hat ein höheres Maß
 als ein Term, das entweder in fast jeden Dokument oder sehr geringfügig
 auftaucht.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Die Instanz vom Obekt 
\emph on
TfidfTransformer()
\begin_inset Foot
status open

\begin_layout Plain Layout

\emph on
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.Tf
idfTransformer.html
\end_layout

\end_inset

, 
\emph default
welches von der Bibiliothek 
\emph on
sklearn
\emph default
 zur Verfügung gestellt wird, berechnet das Tf-idf-Maß anhand dem zuvor
 berechneten Matrix mit dem Vokabular für jeden Text im Dokumentenkorpus
\begin_inset Foot
status open

\begin_layout Plain Layout
Sammlung schriftlicher texte
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "caption={Initalisierung und Anpassung des Tf-idf-Transformierers},captionpos=b"
inline false
status open

\begin_layout Plain Layout

tfidf_transformer = TfidfTransformer() 
\end_layout

\begin_layout Plain Layout

X_train_tfidf = tfidf_transformer.fit_transform(docs)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Anhand des Vokabulars 
\emph on
docs 
\emph default
wird für jedes Wort im Dokumentenkorpus das Tf-idf-Maß berechnet.
\end_layout

\begin_layout Subsection
Naive Bayes Multinomialverteilung
\end_layout

\begin_layout Standard
Naive Bayes Multinominalverteilung ein auf der Bayeschen Regeln basierender
 Klassifikator, der als schnell und einfach zu implementieren gilt und speziell
 in der Textklassifikation Verwendung findet.
 Der Klassifikator modelliert die Verteilung von Wörter in einem Dokument
 als Multinomial
\begin_inset CommandInset citation
LatexCommand cite
key "RSTK03"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Der Klassifikator schätzt die bedingte Wahrscheinlich eines bestimmten Terms
 t bei einer gegebener Kategorie c als relative Häufigkeit des Terms in
 den Dokumenten der gegebenen Kategorie c
\begin_inset CommandInset citation
LatexCommand cite
key "M08"
literal "false"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(t|c)=\frac{T_{ct}}{\sum_{t'\epsilon v}T_{ct'}}
\]

\end_inset


\end_layout

\begin_layout Standard
Die Motivation des Algorithmus ist die Berücksichtigung der Variationen
 von der Summe der Vorkommen des Terms t in den Dokumenten der Kategorie
\end_layout

\begin_layout Standard
c.
 Um Diesen Algorithmus auf die Trainingsdaten anzupassen, wird das 
\emph on
MultinomialNB()
\emph default

\begin_inset Foot
status open

\begin_layout Plain Layout
https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.
html
\end_layout

\end_inset

 Objekt verwendet, welches die Tf-idf-Maße des Vokabulars und die dazugehörigen
 Kategorien entgegennimmt.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "caption={Initialisierung und Anpassen des Naive Bayes Multinomialverteiler},captionpos=b"
inline false
status open

\begin_layout Plain Layout

text_clf = MultinomialNB()
\end_layout

\begin_layout Plain Layout

text_clf.fit(X_train_tfidf, y_train)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Um die Präzision des Modells zu testen wird eine Validierung des Modells
 vorgenommen, welche anhand der Testdaten durchgeführt wird.
 Das Modell wird durch die Testdaten nicht weiter trainiert werden, da für
 diesen Zweck ausschließlich die Trainingsdaten benutzt werden und bereits
 vollständig trainiert wurden ist.
\end_layout

\begin_layout Subsection
Validierung
\end_layout

\begin_layout Standard
Der Validierungsprozess ist mit der Evaluation des Modells gleichzusetzen
 und dient als Bewertungsmaßstab für die Präzision des Klassifikators.
 Wie bei dem Trainingsdatensatz sind die Testdaten ebenfalls kategorisiert,
 was bedeutet, dass die Kategorie der Dokumententexte bekannt ist.
 Resultierend kann der Klassifikator die Testdaten ohne Einbeziehung der
 Kategorie klassifizieren und anschließend und mit der richtigen Kategorie
 der Testdatenbeispiele abgeglichen werden, um so die Präzision des Models
 zu testen.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Durch die Funktion
\emph on
 predict() 
\emph default
werden die Testdaten an den Klassifikator übergeben, wodurch anschließend
 einen Report erstellt werden kann.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "caption={Evaluation des Textklassifkators},captionpos=b"
inline false
status open

\begin_layout Plain Layout

y_pred = text_clf.predict(X_test)
\end_layout

\begin_layout Plain Layout

classification_report(y_test, y_pred)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Der Report wird mit der Funktion 
\emph on
classification_report()
\emph default

\begin_inset Foot
status open

\begin_layout Plain Layout
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_re
port.html
\end_layout

\end_inset

 generiert und liefert eine Auswertung in der Form, die in Abbildung gezeigt
 wird.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename images/report.PNG

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:report"

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Auswertungsreport
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Der Report zeigt verschiedene Ergebnisse, die aus der Evaluation des Models
 hervorgegangen sind.
 Die Bedeutung dieser Werte können anhand der Dokumentation
\begin_inset Foot
status open

\begin_layout Plain Layout
https://www.scikit-yb.org/en/latest/api/classifier/classification_report.html
\end_layout

\end_inset

 von sklearn abgeleitet und zusammengefasst werden.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\emph on
precision 
\emph default
ist das Maß eines Klassifikators wie gut er negative Ergebnisse von positiven
 Ergebnisse unterscheiden kann.
 
\emph on
recall 
\emph default
ist die Fähigkeit eines Klassifikators korrekte Annahmen in den positiven
 Ergebnissen zu treffen.
 Der
\emph on
 f1-score
\emph default
 ist ein Bewertungsmaß, welches aus der Präzision und dem Rückruf gewichtet
 wurden ist.

\emph on
 support 
\emph default
gibt die jeweilige Anzahl der tatsächlichen Vorkommen der Klassen im Datensatz
 zurück.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Die Verwirrung Matrix bildet die Positiven und negativen Ergebnisse und
 stellt sie als Heatmap
\begin_inset Foot
status open

\begin_layout Plain Layout
Diagramm mit Farbcodierung
\end_layout

\end_inset

 dar.
 Sie visualisiert die Bewertung der Qualität der Ausgabe des Klassifikators.
 Bestenfalls verläuft bei der Verwirrung-Matrix eine 
\begin_inset Quotes gld
\end_inset

helle
\begin_inset Quotes grd
\end_inset

 Diagonale von links oben nach rechts unten, da die Werte auf der Diagonalen
 die positiven Ergebnisse repräsentieren.
 Sie wurde anhand von
\emph on
 scikit-learn
\emph default

\begin_inset Foot
status open

\begin_layout Plain Layout
https://scikit-learn.org
\end_layout

\end_inset

 erstellt.
 In der Dokumentation
\begin_inset Foot
status open

\begin_layout Plain Layout
https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matr
ix.html
\end_layout

\end_inset

 beschrieben, dass
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename images/confusion_matrix.PNG

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:conf_ma"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Verwirrung-Matrix
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Interpretation
\end_layout

\begin_layout Standard
Insgesamt werden die Rechnungsbelege und Regressbelege sicher (96%) als
 positiv erkannt.
 Des Weiteren werden die Belege, die als positiv gewertet sind, richtig
 (96%) prognostiziert.
 Die tatsächlichen Vorkommen der Belege ist im geringen Maße unausgeglichen
 und sollte komplett balanciert sein um strukturelle Schwächen zu vermeiden.
 Da der Unterschied jedoch sehr gering ist, sollte das keine großen Auswirkungen
 die Evaluation haben.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
In der abgebildeten Verwirrung Matrix 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:conf_ma"
plural "false"
caps "false"
noprefix "false"

\end_inset

 zeigt die Ergebnisse zur Erkennung zweier Klassen Rechnungsbeleg und Regressbel
eg auf.
 Mit einer Gesamtanzahl von 280 Vorhersagen wurden insgesamt 122 Stichproben
 als Rechnungsbeleg und 158 Stichproben als Regressbeleg identifiziert.
 In der Realität gibt es 142 Regressbelege und 138 Rechnungsbelege, was
 bedeutet dass 18 Regressbelege fälschlicherweise als Rechnungsbeleg und
 2 Rechnungsbelege als Regressbelege identifiziert wurden sind.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
endinput
\end_layout

\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "./bib/thesis"

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
