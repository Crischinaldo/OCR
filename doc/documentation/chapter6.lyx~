#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrbook
\options bibliography=totoc,listof=totoc,BCOR=5mm,DIV=12
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding iso8859-1
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 0
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine biblatex
\cite_engine_type authoryear
\biblio_style plainnat
\biblio_options backend=biber
\biblatex_bibstyle alphabetic
\biblatex_citestyle alphabetic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
chapter{Entwicklung des Textklassifikators}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chap_architecture_classificator"

\end_inset


\end_layout

\begin_layout Standard
Dieses Kapitel befasst sich mit der Entwicklung des Klassifikators und ist
 in den zwei Kapiteln Architektur und Implementierung gegliedert.
\end_layout

\begin_layout Section
Technologien und Frameworks
\end_layout

\begin_layout Standard
Jupyter Notebook, Pandas, Keras, Tensorflow...
\end_layout

\begin_layout Subsection
Jupyter Notebook
\end_layout

\begin_layout Standard
Jupyter Notebook bietet die Möglichkeit in einer interaktiven Umgebung Programmc
ode parallel durchlaufen zu können.
 Es können Ergebisse als Live-Berechnung oder im festen Format an die Zellen
 weitergereicht werden.
 Dieser Vorteil bietet eine Abschnittsähnliche Ausführung von Code in den
 jeweiligen Zellen.
 Notebooks-Dateien enthalten komplette Aufzeichnungen aller Berechnungen
 und können anderen Jupiter-Nutzern zur Verfügung gestellt werden.
 Jupiter eignet sich sehr gut für das Visualisieren von Berechnungen im
 Data Science Bereich.
\end_layout

\begin_layout Subsection
Pandas
\end_layout

\begin_layout Standard
Das Dataframe von Pandas ist eine Datenstruktur welche auf dem Schlüssel-Wert-Mu
ster aufgebaut ist.
 Sie tabellarisiert Schlüssel und die jeweiligen Werte in einem Dataframe,
 welches sich gut zum präsentierten von den beinhalteten Daten eignet.
\end_layout

\begin_layout Subsection
nltk
\end_layout

\begin_layout Section
Organisation der Daten
\end_layout

\begin_layout Standard
Unter der Organisation der Daten ist sowohl die Formatierung als auch die
 Strukturierung von den zur Verfügung stehenden Datensätzen gemeint.
 Um das Model mit Überwachtem Lernen zu trainieren müssen die Daten so strukturi
ert sein, dass das 
\emph on
Labels
\emph default
 (Ergebnisse) von jeden Datenpunkt bekannt ist.
 Ursprünglich sind die Bilder im PDF-Format in einer Ordnerstruktur gegliedert
 gewesen, die abhängig vom Hochladedatum nach Jahr, Monat und Tag sortiert
 wurden sind.
 Die Zeitangabe für die Textextraktion ist als unrelevant zu werten, da
 hier nur auf der inhaltliche Aspekt eine Rolle spielt.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Subsubsection
Kollektion
\end_layout

\begin_layout Standard
Mit dem Programmcode 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:collect"
plural "false"
caps "false"
noprefix "false"

\end_inset

sind alle Dateien kollektiert, indem rekursiv über jeden Ordner in einem
 bestimmten Ordner
\emph on
 input_dir 
\emph default
alle PDF-Dateien erfasst wurden und in den Ordner 
\emph on
output_dir 
\emph default
verlagert wurden sind.
 Die Merkmalsgenerierung verläuft über das Extrahieren der Texte aus Bilddateien
, weswegen jedes Bild mit der Funktion 
\emph on
convert_from_path()
\emph default
 der externen Bibliothek 
\emph on
pdf2image
\emph default
 zum JPG-Format konvertiert wurden ist.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

def collect_data(input_dir, output_dir):
\end_layout

\begin_layout Plain Layout

    ext = 'jpg'
\end_layout

\begin_layout Plain Layout

    for pdf in glob.glob('{inp}/*/**/*.pdf'.format(inp=input_dir), recursive=True):
       
\end_layout

\begin_layout Plain Layout

        try:             
\end_layout

\begin_layout Plain Layout

        	pages = convert_from_path(pdf)         
\end_layout

\begin_layout Plain Layout

		except (ValueError, Exception):             
\end_layout

\begin_layout Plain Layout

    		pass      
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

    for page in pages:             
\end_layout

\begin_layout Plain Layout

   	 page.save('./{output}/{name}.{ext}'.format(
\end_layout

\begin_layout Plain Layout

             output=output_dir, name=str(uuid.uuid4())),  ext=ext)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Kollektieren der Daten
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "alg:collect"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Die rekursive Suche verlief über glob (
\emph on
Unix style pathname pattern expansion
\emph default
), einer Bibliothek die alle Pfadnamen nach einem spezifischen Muster nach
 Unix-Regeln durchsucht.
 Die Funktion 
\emph on
convert_from_path() 
\emph default
gibt eine Liste von Seiten als Bild-Objekte zurück, da PDF-Dokumente logischerwe
ise eine Mehrzahl an Seiten aufweisen können.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Beim Analysieren der Bilder stellte sich heraus, dass es folgende grundlegende
 Typen gibt:
\end_layout

\begin_layout Itemize
Schadenfotos
\end_layout

\begin_layout Itemize
Dokumente
\end_layout

\begin_deeper
\begin_layout Itemize
Rechnungsbelege
\end_layout

\begin_layout Itemize
Briefe
\end_layout

\begin_layout Itemize
Lieferscheine
\end_layout

\end_deeper
\begin_layout Subsubsection
Sortierung
\end_layout

\begin_layout Standard
Eine interessante Möglichkeit die Bilder zu Sortieren ist nach Schlüsselwörtern
 in den Texten der Dokumente zu suchen und sie abhängig davon in bestimmte
 Ordner zu sortieren.
 Durch eine Textextrahierung jedes Bildes gelang es die extrahierten Texte
 nach bestimmten Schlüsselwörtern zu durchsuchen und sie dadurch zu sortieren.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Section
Formatierung der Daten
\end_layout

\begin_layout Standard
Um Die Daten für die Klassifizierung vorzubereiten mussten die Daten in
 einem bestimmtes Format gebracht werden.
 Im Programmcode 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:formatting"
plural "false"
caps "false"
noprefix "false"

\end_inset

 wird vom Ordner, der durch den Parameter 
\emph on
input_dir 
\emph default
projeziert ist, von der Funktion 
\emph on
extract_text() 
\emph default
der Text extrahiert.
 Vor dem Extrahierungsprozess wird das Bild mit der Funktion 
\emph on
preprocess() 
\emph default
binarisiert, um die Präzision der Texterkennung zu erhöhen.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

def preprocess(im):     
\end_layout

\begin_layout Plain Layout

	_ ,im = cv2.threshold(np.array(im),127,255,cv2.THRESH_BINARY)     
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

	return Image.fromarray(im)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

def extract_text(im):     
\end_layout

\begin_layout Plain Layout

	im = Image.open(im)     
\end_layout

\begin_layout Plain Layout

	im = preprocess(im)          
\end_layout

\begin_layout Plain Layout

	extracted = pytesseract.image_to_string(im, output_type=Output.DICT, lang='deu')
     
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

	return extracted
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

def texts_to_list(input_dir, label):
\end_layout

\begin_layout Plain Layout

	res = []     
\end_layout

\begin_layout Plain Layout

	ext = "jpg"     
\end_layout

\begin_layout Plain Layout

	images = len(glob.glob('{dir}/*.{ext}'.format(dir=input_dir, ext=ext))   
\end_layout

\begin_layout Plain Layout

	for im, i in zip(images, range(len(images))):
\end_layout

\begin_layout Plain Layout

    	extracted_text = extract_text(im)         
\end_layout

\begin_layout Plain Layout

		extracted_text['id'] = i +1         
\end_layout

\begin_layout Plain Layout

		extracted_text['label'] = label         
\end_layout

\begin_layout Plain Layout

		res.append(extracted_text)         
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

	return res
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Formatierung zu JSON
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "alg:formatting"

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Die Methode 
\emph on
texts_to_list
\series bold
\emph default
 
\series default
fügt dem Dictionary, welches durch 
\emph on
image_to_string()
\emph default
 zurückgegeben wurden ist, eine ID und das übergebene Label hinzu und returned
 eine Liste von Dictionaries.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Mit dem Aufruf 
\emph on
json.dump() 
\emph default
wurden die Listen in ein Json-Array überführt und in einer Json-Datei abgelegt
 und auf der Festplatte gespeichert.
 Mit Pandas wurden die Json-Dateien wiederrum in einen Pandas-Dataframe
 überführt.
 Hierzu reichte der Aufruf mit
\emph on
 pd.read_json().
 
\emph default
Die Dataframes die korrespondieren zu den Labels stehen werden konkateniert,
 was dazu führt, dass sich alle Texte mit dem im Bezug stehenden Label in
 einer Schlüssel-Wert-Struktur in Form eines Dataframes befinden.
 Die Einträge werden zudem randomisiert.
\end_layout

\begin_layout Section
Aufbereitung der Daten
\end_layout

\begin_layout Standard
stopwords, tokenizing, lower case, stemming
\end_layout

\begin_layout Subsection
normalisierung der Daten
\end_layout

\begin_layout Standard
Zum Normalisieren wurden einige Verfahren angewandt, um die Genauigkeit
 des Klassifikators zu erhöhen
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

stemmer = LancasterStemmer() 
\end_layout

\begin_layout Plain Layout

lemmatizer = WordNetLemmatizer() 
\end_layout

\begin_layout Plain Layout

p = inflect.engine()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# remove special characters 
\end_layout

\begin_layout Plain Layout

df['text'] = df['text'].apply(lambda x: re.sub("(
\backslash

\backslash
W)+"," ",x))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# remove punctuation 
\end_layout

\begin_layout Plain Layout

df['text'].apply(lambda x: re.sub(r'[^
\backslash
w
\backslash
s]', '', x))   
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

# tokenize  
\end_layout

\begin_layout Plain Layout

df['text'] = df['text'].apply(lambda x:nltk.word_tokenize(x))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# to lower case 
\end_layout

\begin_layout Plain Layout

df['text'].apply(lambda x: [word.lower() for word in x])   
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# filter special characters 
\end_layout

\begin_layout Plain Layout

df['text'].apply(lambda x: [word.lower() for word in x])    
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# filter stopwords 
\end_layout

\begin_layout Plain Layout

df['text'].apply(lambda x: [item for item in x if item not in stopwords.words('ger
man')])
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# remove punctuation 
\end_layout

\begin_layout Plain Layout

df['text'].apply(lambda x: [re.sub(r'[^
\backslash
w
\backslash
s]', '', word) for word in x if word != '']) 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Remove non-ASCII characters from list of tokenized words 
\end_layout

\begin_layout Plain Layout

df['text'].apply(lambda x: [unicodedata.normalize('NFKD', word).encode('ascii',
    
\end_layout

\begin_layout Plain Layout

                            'ignore').decode('utf-8', 'ignore') for word
 in x]) 
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

# Replace all interger occurrences in list of tokenized words with textual
 representation df['text'].apply(lambda x: [p.number_to_words(word) for word
 in x if word.isdigit()]) 
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

# stemming 
\end_layout

\begin_layout Plain Layout

df['text'].apply(lambda x: [stemmer.stem(word) for word in x])  
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# lemmatizing 
\end_layout

\begin_layout Plain Layout

df['text'].apply(lambda x: [lemmatizer.lemmatize(word, pos='v') for word in
 x])    
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Normalisierung
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Trainings- und Testdaten
\end_layout

\begin_layout Standard
Die Gesamten Daten müssen unter Trainings- und Testdaten unterteilt werden.
 Die Daten werden mit den Mengenverhältnis 80:20 aufgeteilt, da es sich
 im Bereich des Machine Learning als am effektivsten erwiesen hat.
\end_layout

\begin_layout Section
Wahl des Models
\end_layout

\begin_layout Subsection
countvektorizer
\end_layout

\begin_layout Subsection
frequency times inverse document-frequency
\end_layout

\begin_layout Standard
TF: Term Frequency, which measures how frequently a term occurs in a document.
 Since every document is different in length, it is possible that a term
 would appear much more times in long documents than shorter ones.
 Thus, the term frequency is often divided by the document length (aka.
 the total number of terms in the document) as a way of normalization:
\end_layout

\begin_layout Standard
TF(t) = (Number of times term t appears in a document) / (Total number of
 terms in the document).
\end_layout

\begin_layout Standard
IDF: Inverse Document Frequency, which measures how important a term is.
 While computing TF, all terms are considered equally important.
 However it is known that certain terms, such as "is", "of", and "that",
 may appear a lot of times but have little importance.
 Thus we need to weigh down the frequent terms while scale up the rare ones,
 by computing the following:
\end_layout

\begin_layout Standard
IDF(t) = log_e(Total number of documents / Number of documents with term
 t in it).
\end_layout

\begin_layout Standard
bayes
\end_layout

\begin_layout Standard
term-frequency times inverse document-frequency
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
endinput
\end_layout

\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "./bib/thesis"

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
