**** BACHELORARBEIT ****

**** FUNCTIONS ****

np.stack: stacks x 1D arrays into a matrix with shapes of given arrays
np.vstack: stack x arrays rowwise
np.hstack: stack x arrays columnwise



**** QUELLEN ****

IAM-Dataset
http://www.fki.inf.unibe.ch/databases/iam-handwriting-database

Handwritten text recognition in historical documents / von Harald Scheidl
https://repositum.tuwien.ac.at/obvutwhs/content/titleinfo/2874742

allg.: https://scholar.google.de

ocr-api: https://ocr.space

deep learning: https://hackernoon.com/latest-deep-learning-ocr-with-keras-and-supervisely-in-15-minutes-34aecd630ed8

text-recognition: best path decoding algorithm

ocr-deep-learning: https://nicholastsmith.wordpress.com/2017/10/14/deep-learning-ocr-using-tensorflow-and-python/

TFANN: A neural network module containing implementations of MLP, and CNN networks in TensorFlow.

https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa

deep learning, tesseract, ocr: https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/

**** OCR ****

The method of extracting text from images is also called Optical Character Recognition (OCR) or sometimes simply text recognition.

bsp. code: https://www.pyimagesearch.com/2017/07/10/using-tesseract-ocr-python/

Automatisierte Texterkennung innerhalb von Bildern

Deep-Learning netzwerke verwendet auf das OCR-Problem - Mit Python und Tensorflow. TFANN als Lib für CNN mit ANNC als Klasse.

OCR Texterkennung über API oder Local?

Generell: Text von jeden möglichen Bild extrahieren. Texterkennungsprozesse sind auch Straßenschilder, Captchas oder Nummerschilder zu lesen.

Attribute der OCR-Aufgaben:

1. Textdichte: sollte möglicht dicht sein. 

2. Textstruktur: Text auf Seite sollte strukturiert sein, meist in strengen Zeilen.
dense (as in printed document) or sparse (As text in the wild)


3. Schriften: Gedruckte Schriften sind einfacher als handgeschriebene Zeichen

4. Zeichentyp: Text kann in anderer Sprache verfasst sein, die sehr unterschiedlich sein kann. 

5. Artefakte: Es können Ausreißer vorkommen.

6. Ort: Einige Aufgaben enthalten aussgeschnittene/zentrierte Texte während sich in anderen Aufgaben Text an zufälligen Stellen im Bild befindet.

Nachdem die Stufe der Zeilen/wörter erkannt wurden ist können wir auf mögliche Lösungen auswählen:

1. Klassische Computer vision techniken

2. Spezialisiertes Deep Learning.

3. Standart Deep Learning (Detection)

**** TESSERACT ****

Arbeitet am besten wenn eine saubere segementierung des Vordergrundtextes zum Hintergrund. Um dies zu gewährleisten neigt man zu domänenspezifische Bildklassifikatoren und detektoren zu trainieren.

Funktioniert nur mit einer bestimmten Sorte von Image Dateien. PDF's müssen in kleine dateien gesplittet werden, damit darauf OCR angewendet werden können.

*** Handwritten Text Recognition ***

Using Keras lstm: https://towardsdatascience.com/build-a-handwritten-text-recognition-system-using-tensorflow-2326a3487cd5

**** Beispiel Repos ****

OCR with lstm RCNN architecture
https://github.com/vinayakkailas/Deeplearning-OCR

OCR-Model
https://github.com/tensorflow/models/tree/master/research/attention_ocr

Single Shot MultiBox Detector
https://github.com/pierluigiferrari/ssd_keras

Keras Example lstm
https://keras.io/examples/image_ocr/


 *** Recurrent neuronal network architecture - LTSM - ***

 Repo: https://github.com/githubharald/SimpleHTR

 https://github.com/githubharald/CTCWordBeamSearch

 LSTM: Populärste Netzarchitektur zur Erkennung von Sequenzstrukturierten Input. Default Architektur zur
 Erkennung von Text. 
 


 ** Ablauf **
 1. CNN: Der Input in Form eines Images wird in CNN Layers gefüttert. Die Schichten sind dazu trainiert um relevante Merkmale aus dem Bild zu extrahieren. Jede Schicht führt 3 Operationen aus:
 1. Faltungsoperation (convolution operation): Ein Filter kernel mit der größte 5x5 wird in den ersten beiden schichten und ein filter mit der größe 3x3 in den letzten drei schichten über das Bild iteriert. Dann wird die RELU Funktion (Aktivierungsfunktion) angewandt. Letztendlich wird mit einem Pooling Layer die Image regionen summiert und es wird ein runterskalierte Version vom Input ausgegeben. Während die Bildgröße um 2 pro Schicht runterskaliert werden, werden feature maps(channels) hinzugefügt sodass die Ausgabe als feature map (oder sequenz) eine größe von 32x256 hat. Anzahl Layer: 5

 2. RNN: die Merkmalssequenz enthält 256 features pro Zeitschritt. Das RNN verbreitet relevante Informationen durch die Sequenz. Die beliebte Long Short-Term Memory (LSTM)-Implementierung von RNN's wird verwendet, da sie Informationen über größere Entferungen verbreiten kann und robustere Trainingseigenschaften bietet als Vanilla-RNN. Die RNN-Ausgabesequenz wird auf eine Matrix der größe 32x80 abgebildet. Der IAM-Datensatz besteht aus 79 verschiedenen Zeichen, für die CTC-Operationen wird zusätzlich ein Zeichen (CTC Blank Label) benötigt, daher gibt es für jeden der 32 Zeitschritte 80 Einträge. Anzahl Layer: 2

 3. CTC: Während des Trainings der NN enthält die CTC die RNN-Ausgangsmatrix und den Grundwahrheitstext und berechnet den Verlustwert (loss value). Beim ABleiten erhält der CTC nur die Matrix und decodiert sie in den entgültigen Text. Sowohl der Grundwahrheitstext als auch der erkannte text können höchstens 32 Zeichen lang sein.


 Quelle: https://en.wikipedia.org/wiki/Connectionist_temporal_classification
 CTC: Typ von Outputs eines neuronalen Netzwerks und gleichzeitig Bewertungsfunktion zum trainieren von RNNs (wiederkehrende Neuronale Netzwerke) wie LSTM-Netzwerke. Diese Netzwerke sind darauf ausgelegt Sequenzprobleme zu lösen wo das Timing variabel ist. Es kann u.A. zum erkennen von handschriftlichen Text genutzt werden. CTC bezieht sich auf die Ausgaben und die Bewertung unabhängig von den Aufbau der neuronales Netzstruktur.
 Der Input ist eine Sequenz aus Beobachtungen und die Ausgaben sind eine Sequenz von Klassen (labels) welche auch leere Ausgaben enthalten können. Die Schwierigkeit des trainings ergibt sich daraus, dass es viel mehr Beobachtungen als Klassen gibt.
 Ein CTC-Netzwerk hat eine kontinuierliche Ausgabe (zb. Softmax), die durch Training angepasst wird, um die Wahrscheinlichkeit einer Klasse zu modellieren.
 CTC-Scores können dann mit dem Back-Propagation-Algorithmus verwendet werden, um die Gewichte des neuronalen Netzwerks zu aktualisieren.

 
 *** Neuronal network infos ***
 
 ** Networks **

 * RNN *

 RNN: Recurrent neuronal networks (wiederkehrende neuronale netzwerke): Eine Art von Netz mit dem Muster in Datensequenzen wie Text, Genomen, Handschrift, gesprochenem Wort oder numerischen Zeitreihendaten erkannt werden können. Die Alrithmen berücksichtigen Zeit und Reihenfolge, sie haben eine zeitliche Dimension.

 Neben den Aufmerksamkeitsmechanismus und der Speichernetzwerken ist es einer der leistungsstärksten und nützlichsten Arten von NN's. RNN's sind sogar auf Bilder anwendbar, die in einer Reihe von Patches zerlegt werden und diese somit als Sequenz behandelt werden können.
 Da wiederkehrende Netzwerke eine bestimmte Art von Gedächnis besitzen wie Menschen.


 Grundlage: Feedforward-Netzwerke. Informationen werden durch eine Reihe von mathematische Operationen geleitet, die an den Knoten des Netzwerkes ausgeführt werden. Eine führt Informationen direkt durch, also wird der Knoten niemals zweimal berührt. Andere Knoten durchlaufen Informationen als eine Schleife. Diese Knoten werden als wiederkehrend bezeichnet.

 Bei Feedforward-Netzwerken werden Eingabebeispiele in das netzwerk eingespeist und in eine Ausgabe umgewandelt.
 Bei überwachten Lernen werden Muster gelernt die bei einer Eingabe signalisieren um es sich ein bestimmtes Objekt handelt. Es wird mit gelabelten Bilder trainiert bis der Fehler beim Erraten der Kategorien minimiert ist. Resultierend werden Knoten so parametisiert, dass die Fehlerrate minimiert ist. Das Netzwerk ist nun in der Lage Daten zu kategorisieren, die es noch nie gesehen hat. Wenn das erste Foto klassifiert wurden ist, hat diese Entscheidung keinen Einfluss auf die Klassifikation des nächsten Bildes.
 Mit anderen Worten das Feedforward-netzwerk hat keine Vorstellung von der zeitlichen Reihenfolge, und die einzige Eingabe, die es berücksichtigt, ist das aktuelle Beispiel, den es gefüttert wurde. Das Netzwerk ist nicht in der Lage vergangende Klassifikationen der Bilder mit in die aktuelle Klassifikation zu berücksichtigen.

 Wiederkehrende Netzwerke achten jedoch nicht nur auf die aktuelle Eingabe, sondern auch das was sie zuvor bereits wahrgenommen haben. Die Entscheidung die ein wiederkehrendes Netz zum Zeitpunkt t-1 getroffen hat, wirkt sich auf die Prognose aus, die es einen Moment später zum Zeitpunkt t treffen wird. Es betrachtet also zwei Eingangsquellen, die Gegenwart und die jüngste Vergangenheit, die zusammen bestimmten, wie sie auf neue Daten reagieren. So handelt auch der Mensch.

 Wiederkehrende netze haben eine Rückkopplungsschleife, die mit ihren frühreren Entscheidungen verbunden ist und die ihre eigenen Ausgaben parallel zu der neuen Eingabe aufnimmt. Einen Art Speicher zu dem neuronalen Netzwerk hinzufügen hat folgenden Grund: In der Sequenz selbst sind Informationen enthalten, und wiederkehrende Netze verwendetn sie, um Aufgaben auszuführen, die ein Feedforward-Netzwerk nicht ausführen kann.

 Diese sequentiellen Informationen bleiben im verborgenen Zustand des RNN's erhalten. Dieses schafft es viele Zeitschritte zu überbrücken während er kaskadiert um die Verarbeitung jeder neuer Eingabe zu beeinflussen. Es werden Korrelationen zwischen Ereignissen gefunden, die durch Momente voneinander getrennt sind. Diese Korrelationen werden als langfristige Abhängigkeiten("long-term dependencies") bezeichnet, da ein zeitlich aktuelles Ereigniss von einem beinflusst wird, welches wiederrum von dem vorherigen Ereigniss beinträchtigt wird. Die erste Klassifikation hat also indirekten Einfluß auf die Entscheidung auf die immer zeitlich aktuelle Klassifikation. 

 Backpropagation über Zeit.

 Der Zweck eines RNN's besteht sequentielle Eingaben genau zu klassifizieren. Wir verlassen uns dabei auf die Rückübertragungen von Fehler und Gradient.

Die Backpropogation bei feed-forward-Netzwerken bewegt sich vom endgültigen Fehler durch die Ausgaben, gewichte und Eingaben jeder verborgenen Schicht (Hidden Layer) in Rückwärtsrichtung, wobei diese Gewichte, die für einen Teil des Fehlers verantwortlich sind, verbessert werden. Durch partiellen Ableitungen werden mithilfe eines Gradienten, der die Lernregel darstellt, die Gewichte nach oben oder unten angepasstn, je nachdem in welche Richtung der Fehler verringert wird.



h t = φ(Wx t + Uh t-1)

 Die Summe der Gewichtseingabe und verdecktem Zustand wird durch die Funktion φ (Phi) - je nach Art der logistischen Sigmoidfunktion oder tanh - berechnet. Diese sind Standartwerkzeuge zum Verdichten sehr großer oder sehr kleiner Werte in einem logistischen Raum sowie zum Errechnen der Gradienten für die Backpropagation.

 h_t beschreibt den verborgenen Zustand im Zeitschritt t. Das ist eine Funktion von der Eingabe im Selben Zeitschritt x_t, welche von der Gewichtematrix W modifiziert wird. Diese wird zu den verborgenen Zustand des vorherrigen Zeitschritts h_t-1 mit der eigenen verborgener-status-zu-verborgenener-status matrix U multiplizert.
 Diese Matrix wird auch transition matrix genannt und ist ähnlich zur Markov chain.

  Durch Gewichtsmatrizen, die als Filter funktionieren, wird ermittelt wie wichtig die sowohl aktuelle Eingabe als auch der vergangende verborgene Zustand ist. Der von ihnen erzeugte Fehler wird durch Backpropagation (Rückausbreitung Rückausbreitung) zurückgegeben und zum Anpassen ihrer Gewichte verwendet, bis der Fehler nicht mehr geringer werden kann. 

Da diese Rückkopplungsschleife zu jedem Zeitschritt in der Reihe auftritt, enthält jeder verborgene Zustand nicht nur spuren des vorherigen verborgenen Zustands sondern auch all jener, die h_t-1 vorausgingen, solange das Gedächtnis oder Speicher bestehen kann. 

Bei einer Reihe von Buchstaben verwendet ein wiederkehrendes Netzwerk das erste Zeichen um die Wahrnehmung des zweiten Zeichens zu bestimmten, sodfass ein intiales q darauf schleißen lässt, das der nächste Buchstabe u ist, während ein initiales t dazu führen kann dass der nächste buchstabe h sein wird.


 Wiederkehrende Netzwerke basieren auf eine Erweiterung der Backpropagation, da hier die Zeit einkalkuliert wird. Somit findet eine ständige Backprogation statt.

 Problem: Nicht für Deep-Learning gedacht, da das ständige Multiplizieren der Matrizen dazu führt dass der Gradient explodiert. Explodierende Gradienten überschätzen kleine Gewichte Stark. In diesen Fall kann das RNN nicht mehr lernen, bzw verlernt. 

 Lösung: LSTM

 Long Short Term Memory Units (LSTM) als Lösung für das Problem des verschwindenden Gradienten vorgeschlagen.
 LSTM's tragen dazu bei, den Fehler zu bewahren, der durch Zeit und Ebenen zurückpropagiert werden kann. Indem sie einen konstanteren Fehler aufrechterhalten, ermöglichen sie es wiederkehrende Netzen über viele Zeitschritte (über 1000) weiterzulernen, wodurch ein Kanal (Channel) geöffnet wird, um Ursachen und Wirkungen aus der Ferne zu verknüpfen. 

 LSTM's enthalten Informationen außerhalb des normalen Flusses des RNN's in einer Gate-Zelle. Infromationen können in eine Zelle gespeichert werden, in diese geschrieben oder gelesen werden. Diese Zelle entscheidet wann Lese-, Schreib- und Löschvorgänge und wann öffnende und schließende Tore möglich sind.
 Diese Gater sind analog und sind somit differenzierbar und für die Backpropagation geeignet.

 diese Gates wirken auf empfangende Signale und blockieren oder leiten Informationen ähnlich wie die Knoten des neuronales Netzwerks mit Gewichtungsstärke und Eingänge weiter, die sie mit ihren eigenen Gewichten filtern. Diese Gewichte, die den Eingangs und den verborgenen Zustand modulieren, werden mit den Lernprozess des RNN's angepasst. Dh. Alle Zellen lernen, wann Daten eingegeben, verlassen oder gelöscht werden dürfen, indem sie iterativ Vermutungen anstellen, Fehler rückgängig zu machen (Backpropagation) und Gewichte über den Gradientenabstieg anzupassen.

 code example:
 https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/



 quelle: https://skymind.ai/wiki/lstm

 * CNN *

 CNN takes no vector but multichanneled image 
 
 ** Layers **
 
 convolutional layer: A filter with a defined 2d-size slides over the complete image. for each image chunk the filter
slides over a dot product is produced. for each produced dot product a scalar is the result. for each filter another
feature map is created. with 6 independent filters it will produce 6 feature maps. Another conv Layer would produce
dot products from the previous conv-layer output containing dot products from the original img.

pooling layer: the function of pooling is to progressively reduce the spartial size of the representration to reduce
the amount of parameters in the network. they operatre on each feature map independently. Max pooling is the most common used technique. with a specific 2d filter it takes the highest weighted neuron
 
Fully connected layer: all neurons in the layer are connected to all neurons in the previous layer.

Relu Layer(aktivierungsfunktion):
 Multilayer perceptrons: fully connected networks - each neuron is connected to each neuron in the next layer
 

 
 